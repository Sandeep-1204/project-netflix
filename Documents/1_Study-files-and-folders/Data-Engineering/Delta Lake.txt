Delta Lake:

1) Why data lake formed?
Data lakes were formed because traditional data storage (like data warehouses) couldn’t handle:

Massive volumes of data (think TBs, PBs).

All types of data – not just structured (like tables), but also semi-structured (JSON, XML) and unstructured (videos, images, logs, etc.).

The need for flexibility – companies wanted to store raw data first, then process it later when needed.

Answer: Data lakes were created to store vast amounts of raw data in any format, enabling scalable and flexible analytics.

2)Tombstoning:

When you delete or update data in a Delta Lake table, the actual data files aren’t deleted right away. Instead:

Delta marks them as “tombstoned” 🏷️ in its transaction log (called _delta_log).

These files are now logically removed, but still physically exist on disk.

This lets Delta support time travel ⏳, rollback, and versioned queries.

------------------------------------------------------------------------------------------------------------------------------------------------

DML in delta lake:

Scenario 1) When deletion vector is off:

As we know, delta logs store the transaction logs of the file, every changes of the file is transaction logged.

When we update the file, the actual file is tombstoned and a new parquet file is created with the new changes. The delta logs holds the changes that are made in the form of json. 

Scenario 2) When deletion vector is on:
Deletion vector: The deletion vector marks the removed rows are deleted without deleting it from actual file.

(When we update the file, we can see 3 parquet files instead of two that is
1 --> Actual file
2 --> Holds only the removed or updated rows
3 --> Collection of actual and removed rows, that is the updated new file.) --> Classic Delta

(Only two files
1 --> Deletion Vector file holds the deleted records
2 --> Changed file with updates
changes are logged in metastore)  --> Next Gen Delta lake (makes it faster and cheaper)

The deletion vector scans the whole file and gets only the deleted or updated rows and changes that row only instead of changing the whole. This helps in the big data handling.

After 10 json logs, when it comes to 11th change it tottaly stores the summary of 10 json in 1 file.

------------------------------------------------------------------------------------------------------------------------------------------------

Optimizations:

1)Data Skipping and Zorder By:
Data Skipping is a performance optimization technique where the engine skips scanning unnecessary data files during query execution.\

In data skipping, smaller partitions are colliced and made into big data partions. And ZorderBy orderby orders the data by our query column So that the querying made easy. And the old small partitions are tombstoned.

2)Liquid clustering:
Liquid Clustering is Delta Lake’s way of automatically organizing data for faster queries without manual optimization like Z-ordering

------------------------------------------------------------------------------------------------------------------------------------------------

Schema changes:

The changes in schema does not create a new parquet file, these changes are stored in delta-log.

REORG command pushes all changes to the parquet file.





